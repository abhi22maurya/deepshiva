# Model Configuration
model:
  architecture: moe
  num_layers: 32
  hidden_size: 4096
  intermediate_size: 11008
  num_attention_heads: 32
  num_key_value_heads: 32  # For GQA
  num_experts: 8
  num_selected_experts: 2  # Top-2 routing
  vocab_size: 32000
  max_position_embeddings: 131072  # 128K context
  rms_norm_eps: 1e-5
  rope_theta: 1000000.0  # For RoPE scaling
  use_parallel_residual: True
  moe_layer_frequency: 2  # Add MoE layer every N layers

# Training Configuration
training:
  batch_size: 4
  micro_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  weight_decay: 0.1
  warmup_steps: 1000
  max_steps: 100000
  lr_scheduler_type: cosine
  min_lr_ratio: 0.1
  max_grad_norm: 1.0

# Dataset Configuration
data:
  train_files:
    - data/processed/train.jsonl
  val_files:
    - data/processed/val.jsonl
  max_seq_length: 131072
  preprocessing_num_workers: 8

# LoRA Configuration (for efficient fine-tuning)
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj"]
  task_type: CAUSAL_LM

# DeepSpeed Configuration
deepspeed:
  zero_stage: 3
  offload_optimizer_device: "cpu"
  offload_param_device: "cpu"
  stage3_prefetch_bucket_size: 5e8
  stage3_param_persistence_threshold: 1e6
  stage3_max_live_parameters: 1e9
  stage3_max_reuse_distance: 1e9

# Logging & Checkpointing
logging:
  log_dir: "./logs"
  checkpoint_dir: "./checkpoints"
  save_steps: 1000
  eval_steps: 500
  save_total_limit: 5
  report_to: "wandb"
  run_name: "deepshiva-moe-base"

# Inference Configuration
inference:
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  max_new_tokens: 2048
  repetition_penalty: 1.1
  do_sample: True
