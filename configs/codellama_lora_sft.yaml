model_name_or_path: /Users/abhishek/DeepShiva/models/pretrained/codellama-7b
dataset: codealpaca
dataset_dir: /Users/abhishek/DeepShiva/data/raw
template: default
finetuning_type: lora
lora_rank: 16
lora_alpha: 16
lora_dropout: 0.1
lora_target: q_proj,k_proj,v_proj,o_proj
output_dir: /Users/abhishek/DeepShiva/results/fine_tuned_models
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-4
num_train_epochs: 3
quantization_bit: 4
logging_steps: 10
save_steps: 1000
do_train: true